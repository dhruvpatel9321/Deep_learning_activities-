{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long-Short Term Memory)\n",
    "\n",
    "Some Understanding of RNN(Recurrent Neural Network)\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "<img src=\"image/img0.png\">\n",
    "When we think about one thing, we don't discard everything before, and then think with a blank brain. Human mind has persistence. Consider such a problem, when we talk to others, we want to predict what this person will say next, usually we need to understand what he said in the previous sentence, and then based on past communication experience, we can predict his next sentence. For example, \"It's raining today, I\" you might guess he would say \"no umbrella\" or \"don't want to go out\". Traditional neural networks cannot do this, and recurrent neural networks (RNNs) can solve the problem of correlation between sequence data.\n",
    "\n",
    "The main purpose of the neural network is a processing cycle and the predicted sequence data, the network configuration information prior to the neural network memory cycle will, later affect the output node information before using its typical structure as shown below, it can be seen cycle The nodes between the hidden layers of the neural network are connected. The input of the hidden layer includes not only the input of the input layer but also the output of the previous hidden layer.\n",
    "\n",
    "<img src=\"image/img1.png\">\n",
    "\n",
    "One disadvantage of the recurrent neural network structure in the figure above is that it uses only the previous information in the sequence to make predictions, and does not use the subsequent information. Because if this sentence is given, \"Teddy Roosevelt was a great President.\" In order to determine whether Teddy is part of a person's name, it is not enough to know only the first two words in the sentence. This is also very useful, because the sentence may also be like, \"Teddy bears are on sale!\". So if only the first three words are given, it is impossible to know exactly whether Teddy is part of a person’s name. The first example is a person’s name, and the second example is not, so you can’t tell just by looking at the first three words.Therefore, BRNN is proposed to solve this problem.BLSTM is a typical representative of BRNN.\n",
    "\n",
    "\n",
    "### LSTM (Long Short-Term Memory Network)\n",
    "\n",
    "\"A lot of factories are opened in a certain place, the air pollution is very serious ... the sky has turned gray\", if our model is trying to predict the last word of this sentence \"gray\", it can not be done based on short-term dependence alone, because if We can't tell whether the sky is \"blue\" or \"gray\" without looking at the \"air pollution is very serious\" above. Therefore, the text gap between the current predicted position and related information may become very large. When this gap becomes large, the simple recurrent neural network will lose the ability to learn such far information. LSTM is used to solve such problems.\n",
    "\n",
    "LSTM network is a special network structure with three \"gates\", which are \"forget gate\", \"input gate\", and \"output gate\" in this order.The figure below shows the network structure and formula of LSTM, where c is the memory cell state, x is the input, and a is the output of each layer.\n",
    "\n",
    "<img src=\"image/img2.png\">\n",
    "\n",
    "Let's explain these three gates separately. Understanding the role of these three gates is also the key to understanding LSTM.\n",
    "\n",
    "**1.Forgotten Gate:**\n",
    "\n",
    "   Effect on: memory cell state\n",
    "\n",
    "   Effect: Selective forgetting of information in memory cells\n",
    "\n",
    "Example: \"She is busy today ... I am\" When predicting \"am\" we have to selectively forget the previous subject \"She\", otherwise a syntax error will occur.\n",
    "\n",
    "**2.Input gate:**\n",
    "\n",
    "   Effect on: memory cell state\n",
    "\n",
    "   Effect: Record new information selectively into new cell states\n",
    "\n",
    "Example: In the above sentence, we will update this subject information to the cell state based on \"I\", so \"am\" will be predicted at the end.\n",
    "\n",
    "**3.Output gate:**\n",
    "\n",
    "   Effect on: input and hidden layer output\n",
    "\n",
    "   Effect: The final output includes both the cell state and the input, and the result is updated to the next hidden layer.\n",
    "   \n",
    "Through these three gates, the LSTM can more effectively decide which information is forgotten and which information is retained. Through the forward transmission diagram of the LSTM, we can see that a cell state can be easily transmitted to a long distance to affect the output, so the LSTM can Solve the learning of long distance information.\n",
    "\n",
    "<img src=\"image/img3.png\">\n",
    "\n",
    "<img src=\"image/img4.png\">\n",
    "\n",
    "\n",
    "## Detail Explaination of LSTM architecture\n",
    "\n",
    "LSTM is a very common and useful algorithm in deep learning, especially in natural language processing.What is the internal structure of the LSTM architecture? First, let's look at the overall framework of LSTM:\n",
    "\n",
    "<img src=\"image/img7.png\">\n",
    "\n",
    "In this picture, there is an LSTM module in the middle, and there are three inputs: $c^{(t-1)}$ , $h^{(t-1)}$ and $x^t$ and then after LSTM, the outputs are $c^t$ , $h^t$ and $y^t$ , where $x^t$ represents the input of this round, $h^{(t-1)}$ represents the state quantity output of the previous round,$c^{(t-1)}$  represents the carrier of a global message in the previous round; then $y^t$ represents the output of this round,$h^t$ represents the state quantity output of this round,$c^t$ represents a global information carrier for this round. So it seems that a general framework of LSTM understands. What does the internal structure of LSTM look like?\n",
    "\n",
    "First, we will $x^t$ and $h^{(t-1)}$ merges into a vector and multiplies by a vector *W* , another layer tanh function to get a vector z :\n",
    "\n",
    "<img src=\"image/img8.png\">\n",
    "\n",
    "same thing, we will $x^t$ and $h^{(t-1)}$ merges into a vector, but our activation function uses *sigmoid* , the diagram is as follows:\n",
    "\n",
    "<img src=\"image/img9.png\">\n",
    "\n",
    "multiply by the matrix $W^f$ , $W^i$ and $W^o$ get $z^f$ , $z^i$ and $z^o$ , then we can use these vectors to $c^{(t-1)}$ to obtain $c^t$ , the formula is:\n",
    "\n",
    "<center>$c^t = z^f . c^{(t-1)} + z^i . z$</center>\n",
    "\n",
    "then get c after, we can get $h^t$ , the formula is:\n",
    "\n",
    "<center>$h^t = z^o . tanh (c^t)$</center>\n",
    "\n",
    "Finally we can get the output of this round $y^t$ , the formula is:\n",
    "\n",
    "<center>$y^t = \\sigma(W'H^t)$</center>\n",
    "\n",
    "In summary, we can get a complete internal structure of the LSTM as shown below:\n",
    "\n",
    "<img src=\"image/img10.png\">\n",
    "\n",
    "With this structure, we can clearly and intuitively see the internal structure of the LSTM. First, the green part represents the input of the round $x^t$ and output $y^t$ ; the blue part indicates the status of the previous round $h^{(t-1)}$ and state quantities output by this round $h^t$ ; the red part represents the information carrier of the previous round $c^{t-1}$ and the information carrier output by this roundct $c^t$.This is a single LSTM unit. We can cascade multiple LSTM units to become our LSTM deep learning network. The diagram is as follows:\n",
    "\n",
    "<img src=\"image/img11.png\">\n",
    "\n",
    "OK, after reading the overall architecture of the LSTM, let's analyze the specific each A part. The reason why the entire LSTM architecture can remember long-term information is mainlycn $c^n$ this state, we can see $c^{t-1}$ to $c^t$ middle only a small amount of information exchange, it is possible to maintain the entire network is passed between LSTM, is as follows c State diagram of t:\n",
    "\n",
    "<img src=\"image/img12.png\">\n",
    "\n",
    "reason why LSTM can memorize long-term and short-term information is because it has a \"gate\" structure to remove and add information to the neuron. \"Gate\" is a method for selectively passing information. The first is theforget gate. The first step in LSTM is to decide what information we need to forget from the neuron state. As shown below, two inputs pass one *sigmoid* function, so the output value is 0 - Between1 , 1 means the information is completely retained, 0 means the information is completely forgotten. Through the forget gate, LSTM can selectively forget some meaningless information. As shown in the box below, this part is the forget gate in LSTM:\n",
    "\n",
    "<img src=\"image/img13.png\">\n",
    "\n",
    "this part can be expressed by the formula:\n",
    "\n",
    "<center>$z^f = \\sigma(W_f.[h_{t-1}, x_t] + b_f)$</center>\n",
    "\n",
    "Then the next step we need to confirm what new information is stored in a state of neurons, this section has two inputs, one *sigmoid* layer determines what value LSTM needs to be updated, a tanh layer creates a new candidate value vector, and this value will be added to the state. Then we need to use these two information to generate the update of the state, called theinput gate. The process is as follows:\n",
    "\n",
    "<img src=\"image/img14.png\">\n",
    "\n",
    "entire process can be used The formula is expressed as:\n",
    "\n",
    "<center>$z^i = \\sigma(W_i.[h_{t-1}, x_t] + b_i)$</center>\n",
    "\n",
    "<center>$z=tanh(W.[h_{t-1}, x_t] + b)$</center> \n",
    "\n",
    "identifying the information that needs to be updated, we can update c variable t is shown in the previous figure $c^{t-1}$ to c \n",
    "can be expressed by the formula:\n",
    "\n",
    "<center>$c^t = z^f. c^{t-1} + z^i.z$</center>\n",
    "\n",
    "in this process,$z^f.c^{t-1}$ represents the previous state information $c^{t-1}$ forgets some of the information to be discarded, and then adds the new candidate value vector of the LSTM system, which is the new round of information of the systemct $c^{t}$\n",
    "\n",
    "After speaking the updated information state, we also need to update the neuron state of the systemh $h^t$, the whole process is shown in the box below:\n",
    "\n",
    "<img src=\"image/img15.png\">\n",
    "\n",
    "This is theoutput gatethat controls the output of the LSTM. The system needs to determine what value to output. This output will also be based on the current state of the neuron. First we use *sigmoid* to determine which parts the neuron needs to output, then, we pass the information of the LSTM system through *tanh* function performs processing, and finally multiplies them to output, which is the new state quantity of LSTM. Plus this part *sigmoid*  is the output of this round $y^t$. Can be wrriten as:\n",
    "<center>$z^o = \\sigma(W_o.[h_{t-1}, x_t] + b_o)$</center>\n",
    "<center>$h_t = z_o.tanh(c^t)$</center>\n",
    "<center>$y_t = sigmoid(W'.h_t)$</center>\n",
    "\n",
    "So the entire LSTM can be divided into these parts as described above, each part has a different role, I hope you got the idea about the architecture of LSTM this can help you thoroughly understand the structure and principle of the LSTM neural network. If there are any omissions in the text, please don't hesitate to suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "\n",
    "### Understand LSTM model\n",
    "\n",
    "1. Long-short term memory model is a special RNN model, which is proposed to solve the problem of gradient dispersion of RNN models. In traditional RNN, the training algorithm uses BPTT(Backpropagation through time). For a long time, the residuals that need to be transmitted will decrease exponentially, causing the network weights to update slowly and fail to reflect the long-term memory effect of the RNN. Therefore, a storage unit is required to store the memory, so the LSTM model is proposed.\n",
    "\n",
    "2. The following two figures show the difference between RNN and LSTM:\n",
    "\n",
    "(1).RNN\n",
    "\n",
    "<img src=\"image/img16.png\">\n",
    "\n",
    "(2). LSTM\n",
    "\n",
    "<img src=\"image/img17.png\">\n",
    "\n",
    "PS:\n",
    "\n",
    "(1).The meaning of some figures is as follows:\n",
    "\n",
    "<img src=\"image/img18.png\">\n",
    "\n",
    "(2).The biggest difference between RNN and LSTM is that there is an additional information conveyor called \"cell state\" at the top of the LSTM, which is actually where the information is stored;\n",
    "\n",
    "3. The core idea of LSTM:\n",
    "\n",
    "(1) Understand the core of LSTM is \"cell state\", which is temporarily called the cell state, which is the top transmission line in the above picture, as follows:\n",
    "\n",
    "<img src=\"image/img19.png\">\n",
    "\n",
    "(2) The cell state can also be understood as a conveyor belt. Personal understanding is actually the memory space in the entire model, which changes with time. Of course, the conveyor belt itself cannot control which information is stored. Control gate (gate);\n",
    "\n",
    "(3) The structure of the control gate is as follows: It is mainly composed of a sigmoid function and a dot multiplication operation; the value of the sigmoid function is between 0-1. The dot multiplication operation determines how much information can be transmitted. When it is 0, it is not transmitted. When When it is 1, all transmission;\n",
    "\n",
    "<img src=\"image/img20.png\">\n",
    "\n",
    "(4) There are 3 control gates in LSTM: input gate, output gate, memory gate;\n",
    "\n",
    "4.LSTM works:\n",
    "\n",
    "(1) forget gate: choose to forget certain past information:\n",
    "\n",
    "<img src=\"image/img21.png\">\n",
    "\n",
    "(2) input gate: remember some information now:\n",
    "\n",
    "<img src=\"image/img22.png\">\n",
    "\n",
    "(3) Merge past and present memories:\n",
    "\n",
    "<img src=\"image/img23.png\">image/\n",
    "\n",
    "(4) output gate: output\n",
    "\n",
    "<img src=\"image/img24.png\">\n",
    "\n",
    "PS: The above is the structure of the standard LSTM. In actual applications, it is often improved slightly according to needs;\n",
    "\n",
    "5.Improvement of LSTM\n",
    "\n",
    "(1) peephole connections: add a signal of cell state to the input of each gate\n",
    "\n",
    "<img src=\"image/img25.png\">\n",
    "\n",
    "(2) coupled forget and input gates: merge forget and input gates\n",
    "\n",
    "<img src=\"image/img26.png\">\n",
    "\n",
    "(B) LSTM model derivation\n",
    "\n",
    "<img src=\"image/img27.png\">\n",
    "\n",
    "1. The idea of the LSTM model is to replace each hidden unit in the RNN with a cell with a memory function (as shown in the figure above), and the rest is the same as the RNN;\n",
    "\n",
    "2. The composition of each cell is as follows:\n",
    "\n",
    "(1) Input node (gc): As in RNN, it accepts the output of the hidden node at the previous point in time and the current input as input, and then passes a tanh activation function;\n",
    "\n",
    "(2) Input gate (ic): It plays the role of controlling input information. The input of the gate is the output of the hidden node at the previous point in time and the current input. The activation function is sigmoid (the reason is that the output of sigmoid is between 0-1. Multiplying the output of the input gate by the output of the input node can control the amount of information);\n",
    "\n",
    "(3) Internal state node (sc): The input is the current input filtered by the input gate and the internal state node output at the previous time point, as shown in the formula in the figure;\n",
    "\n",
    "(4) Forget gate (fc): It plays the role of controlling internal state information. The input of the gate is the output of the hidden node at the previous point in time and the current input. The activation function is sigmoid (the reason is that the output of sigmoid is 0-1.) In the meantime, the output of the internal state node and the output of the forget gate can be used to control the amount of information);\n",
    "\n",
    "(5) Output gate (oc): It plays the role of controlling output information. The input of the gate is the output of the hidden node at the previous point in time and the current input. The activation function is sigmoid (the reason is that the output of sigmoid is between 0-1 Multiplying the output of the output gate by the output of the internal state node can control the amount of information);\n",
    "\n",
    "3. The calculation of the LSTM layer can be expressed as follows (several cells make up an LSTM layer):\n",
    "\n",
    "<img src=\"image/img28.png\">\n",
    "\n",
    "PS: Wih in Equation 1 should be changed to Wgh; circles represent dot multiplication;\n",
    "\n",
    "4. The LSTM model with 2 cells is as follows:\n",
    "\n",
    "<img src=\"image/img29.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM\n",
    "\n",
    "## What is LSTM and BiLSTM?\n",
    "\n",
    "LSTM stands for Long short-term memory, which is a type of **RNN(Recurrent neural network)**. Because of its design characteristics, LSTM is too useful for modeling the time series data, such as text data.BiLSTM is the abbreviation of Bi-directional Long Short-Term Memory, which is a combination of forward LSTM and backward LSTM. Both are often used to **model contextual information** in natural language processing tasks.\n",
    "\n",
    "<img src=\"Image/img30.png\">\n",
    "\n",
    "## Why use LSTM and BiLSTM?\n",
    "\n",
    "Combining the representations of words into the representations of sentences, you can use the addition method, that is, add all the representations of the words, or average them, but these methods do not take into account the order of words in the sentence. Such as the sentence ***\"I don't think he is good\"***. The word \"no\" is a negation of the following \"good\", that is, the emotional polarity of the sentence is derogatory. The LSTM model can better capture the long-distance dependencies. Because LSTM can learn what information to remember and what information to forget through the training process.\n",
    "\n",
    "But there is still a problem in modeling sentences with LSTM: it is impossible to encode information from back to front.In more fine-grained classification, such as the five classification tasks for strong meaning, weak meaning, neutral, weak derogation, and strong derogation, attention needs to be paid to the interaction between affective words, degree words, and negative words . For example, \"This restaurant is too dirty to be good, not as good as next door\". Here, \"No\" is a modification of the degree of \"dirty\". BiLSTM can better capture the two-way semantic dependency.\n",
    "\n",
    "\n",
    "The thing involved in Bidirectional Recurrent neural networks(RNN) is preety starightforward.Which involves in making an exact copy of the first recurrent layer in the network then providing the input sequence as it is the input of the first layer and providing the reversed copy of a input sequence to the replicated layer. This get the better of the limitations of the traditional RNN. BRNN(Bidirectional recurrent neural network), which can be trained using all avaiable input information in the past and future of the particular time-step. Split of the state neurons in regular Recurrent neural network(RNN) is responsible for the states(which is in positive time direction) and the part of the backward states(which is in negative time direction).\n",
    "\n",
    "In speech recognition domain a context of whole utterance is used to explain what is being said rather than the linear interpretation thus the input squence is feeded bi-directionally. To be accurate, time steps in the input sequence are processed one at a time, but the network steps through the sequence in both direction at the same time.\n",
    "\n",
    "## Train the bidirectional LSTM on the imdb classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train...\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 176s 7ms/step - loss: 0.4154 - accuracy: 0.8058 - val_loss: 0.3367 - val_accuracy: 0.8517\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.2249 - accuracy: 0.9124 - val_loss: 0.3665 - val_accuracy: 0.8516\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 170s 7ms/step - loss: 0.1337 - accuracy: 0.9522 - val_loss: 0.4526 - val_accuracy: 0.8388\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.5732 - val_accuracy: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c0a8490908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU(Gated Recurrent Unit) Network\n",
    "\n",
    "With the widespread application of LSTMs in natural language processing, especially text classification tasks, people have gradually discovered that <span style=\"color: red\">LSTMs have</span> the disadvantages of <span style=\"color: red\">long training time, many parameters, and complex internal calculations</span>. Cho et al. In 2014 further proposed a simpler GRU model that combines the unit state and hidden layer state of the LSTM with some other changes. The forget gate and the input gate are combined into a single update gate . It also mixes cell states and hidden states . The GRU replaces the forget gates and inputs in the LSTM with update gates. Merging the cell state and the hidden state ht, the method of calculating new information at the current moment is different from that of LSTM.\n",
    "\n",
    "The GRU model is a model that maintains the LSTM effect, has a simpler structure, fewer parameters, and better convergence. The GRU model consists of an update gate and a reset gate.\n",
    "\n",
    "## Update and reset gates\n",
    "\n",
    "<span style=\"color: red\">A moment before the output of the hidden layer of the current degree of influence on the hidden layer by updating door control, update value the greater the greater the impact of the door hidden layer output current before a timing of the hidden layer;</span>\n",
    "\n",
    "<span style=\"color: red\">The extent to which the hidden layer information at the previous moment is ignored is controlled by the reset gate . The smaller the value of the reset gate, the more it is ignored. GRU structure is more streamlined.</span>\n",
    "\n",
    "One of the reasons for using LSTM is to solve the problem that the Gradient errors of the RNN Deep Network accumulate too much, so that Gradient returns to zero or becomes infinity, so the optimization cannot be continued. The construction of the GRU is simpler: it has one less gate than the LSTM, so there are fewer matrix multiplications. GRU can save a lot of time when the training data is large. GRU, which simplifies the calculation method (simplifies the calculation), and also avoids the disappearance of the gradient to optimize the LSTM.\n",
    "\n",
    "##  GRU model\n",
    "\n",
    "Unlike LSTM, GRU has only two gates, namely update gate and reset gate , namely $z_t$ and $r_t$ in the figure.\n",
    "\n",
    "The update gate is used to control the degree to which the state information of the previous moment is brought into the current state. The larger the value of the update gate is, the more state information is brought into the previous moment.\n",
    "\n",
    "The reset gate is used to control the degree of ignoring the state information of the previous moment. The smaller the value of the reset gate, the more it is ignored.\n",
    "\n",
    "<img src=\"Image/img31.png\">\n",
    "\n",
    "## Forward  communication\n",
    "\n",
    "$r_t = \\sigma(W_r .[h_{(t-1)}, x_t])$\n",
    "\n",
    "$z_t = \\sigma(W_z .[h_{(t-1)}, x_t])$\n",
    "\n",
    "$\\tilde{h_t}$ =$tanh(W_{\\tilde h} .[r_t * h_{t-1} , x_t])$\n",
    "\n",
    "$h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde {h_t}$\n",
    "\n",
    "$y_t = \\sigma(W_o . h_t)$\n",
    "\n",
    "# The Difference in between LSTM and GRU\n",
    "\n",
    "- GRU parameters are less than LSTM, so it is easy to converge. In the case of a large data set, the expression performance of LSTM is still better than GRU.\n",
    "\n",
    "- The performance of GRU and LSTM is similar on general data sets.\n",
    "\n",
    "- Structurally, the GRU has only two gates(update and reset), and the LSTM has three gates (forget, input, output). The GRU directly passes the hidden state to the next unit, and the LSTM uses the memory cell to pass the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
